{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ee61bdc",
   "metadata": {},
   "source": [
    "---\n",
    "# SUMMER SCHOOL 2022:\n",
    "## Text mining and Natural Language Processing for Computational Social Sciences\n",
    "\n",
    "### SocialComQuant Project - Online Teaching Module: Text Representation Learning\n",
    "\n",
    "### AndrÃ© Panisson\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dab7a12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b4dc21",
   "metadata": {},
   "source": [
    "# 1. Traditional Approaches to Text Classification\n",
    "\n",
    "In this notebook, we're going to experiment with a few \"traditional\" approaches to text classification. These approaches pre-date the deep learning revolution in Natural Language Processing, but are often quick and effective ways of training a text classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2416e9ca",
   "metadata": {},
   "source": [
    "## 1.1 The Dataset: IMDB\n",
    "\n",
    "IMDB Dataset is a large dataset of movie reviews collected from the Internet Movie DataBase.\n",
    "\n",
    "It is very useful for training text classification models for sentiment analysis. It contains 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.\n",
    "\n",
    "Main reference: http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "We will use the `dataset` package from ðŸ¤— (https://HuggingFace.co) to download and use the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96667e12",
   "metadata": {},
   "source": [
    "We will limit both training and test sets to a smaller subset, so we can run this exercise in less time.\n",
    "\n",
    "If you want, you can remove these limits and check later the results with a bigger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "932e5c6c-a32f-4a31-a382-9a26cf9fea38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/panisson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n",
      "Loading cached split indices for dataset at /home/panisson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-cc7bf445f84ad2a1.arrow and /home/panisson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-6987fd269768b1cf.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb = load_dataset(\"imdb\", split=\"train\")\n",
    "imdb = imdb.train_test_split(train_size=5000, test_size=5000, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3be8b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_documents = imdb[\"train\"]['text']\n",
    "train_target = imdb[\"train\"]['label']\n",
    "\n",
    "test_documents = imdb[\"test\"]['text']\n",
    "test_target = imdb[\"test\"]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f0dd5f0-6aa7-48fd-ad21-ab5b314dbc27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<br /><br />How this film ever got a 6 star average is beyond me. The script is so banal, and frankly an insult to whomevers life it is based upon. The cinematography comes straight from the slick world of advertising, and the talented Ridley Scott should be ashamed. Demi Moore however, shows none a surprise by participating in this film, if one looks at her tracklist. All in all, a \"high concept\" style film that even Don Simpson would be ashamed of.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_documents[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04568ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8118602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 2490, 0: 2510})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168bcefd",
   "metadata": {},
   "source": [
    "## 1.2 Preprocessing\n",
    "\n",
    "The first step in the development of any NLP model is text preprocessing. This means we're going to transform our texts from word sequences to feature vectors. These feature vectors contain their values for each of a large number of features.\n",
    "\n",
    "### The BoW (Bag of Words) approach\n",
    "In this experiment, we're going to work with so-called \"bag-of-word\" approaches. Bag-of-word methods treat every text as an unordered collection of words (or optionally, ngrams), and the raw feature vectors simply tell us how often each word (or ngram) occurs in a text. In Scikit-learn, we can construct these raw feature vectors with the CountVectorizer, which tokenizes a text and counts the number of times any given text contains every token in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8d7f5f2-cf75-46ff-9be7-16cd5a81f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87af5bf2-a77e-4ebf-bfb7-292789024b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "C = cv.fit_transform(train_documents)\n",
    "feature_names = cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af1d56b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 4), ('film', 3), ('is', 3), ('and', 2), ('of', 2), ('br', 2), ('all', 2), ('this', 2), ('in', 2), ('be', 2), ('ashamed', 2), ('it', 1), ('to', 1), ('at', 1), ('one', 1), ('that', 1), ('script', 1), ('an', 1), ('life', 1), ('if', 1), ('would', 1), ('me', 1), ('so', 1), ('got', 1), ('comes', 1), ('from', 1), ('ever', 1), ('however', 1), ('her', 1), ('how', 1), ('world', 1), ('by', 1), ('talented', 1), ('upon', 1), ('participating', 1), ('even', 1), ('don', 1), ('star', 1), ('average', 1), ('beyond', 1), ('banal', 1), ('frankly', 1), ('insult', 1), ('whomevers', 1), ('based', 1), ('cinematography', 1), ('straight', 1), ('slick', 1), ('advertising', 1), ('ridley', 1), ('scott', 1), ('should', 1), ('demi', 1), ('moore', 1), ('shows', 1), ('none', 1), ('surprise', 1), ('looks', 1), ('tracklist', 1), ('high', 1), ('concept', 1), ('style', 1), ('simpson', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted([(feature_names[i], C[10,i]) for i in C[10].nonzero()[1]], key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b1257b",
   "metadata": {},
   "source": [
    "### TF-IDF weighting\n",
    "\n",
    "The above raw counts are not very informative yet. This is because the raw feature vectors of most texts in the same language will be very similar. For example, most texts in English contain many instances of relatively uninformative words, such as a, the or be. Instead, what we're interested in are words like _amazing_ or _terrible_: words that occur often in one text, but not very often in the corpus as a whole. Therefore we're going to weight all features by their tf-idf score, which counts the number of times every token appears in a text and divides it by (the logarithm of) the percentage of corpus documents that contain that token.\n",
    "\n",
    "**Term Frequency(TF)** â€“ The number of occurrences of a word in a document divided by a total number of terms in a document is referred to as Term Frequency. For example, I have to find the Term frequency of _terrible_ in the below sentence then it will be 1/4. It says how frequently a particular word occurs in a particular document.\n",
    "```\n",
    "This movie is terrible.\n",
    "```\n",
    "\n",
    "**Inverse Document Frequency** â€“ Total number of documents in corpus divided by the total number of documents with term T in them and taking the log of a complete fraction is inverse document frequency. If we have a word that comes in all documents then the resultant output of the log is zero. The Scikit-learn implementation uses a little bit different implementation because if it becomes zero then the contribution of the word is ignored so they add one in the resultant and because of which you can observe the values of TFIDF a bit high. If a word comes only a single time then IDF will be higher.\n",
    "\n",
    "This weighting is performed by Scikit-learn's TfidfTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1d2868f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ashamed', 0.34), ('whomevers', 0.24), ('tracklist', 0.24), ('demi', 0.22), ('ridley', 0.21), ('participating', 0.2), ('advertising', 0.2), ('simpson', 0.19), ('banal', 0.19), ('slick', 0.18), ('moore', 0.17), ('insult', 0.16), ('scott', 0.15), ('frankly', 0.15), ('concept', 0.14), ('upon', 0.13), ('talented', 0.13), ('surprise', 0.13), ('straight', 0.13), ('film', 0.13), ('average', 0.13), ('none', 0.12), ('cinematography', 0.12), ('beyond', 0.12), ('the', 0.11), ('style', 0.11), ('star', 0.11), ('based', 0.11), ('shows', 0.1), ('looks', 0.1), ('high', 0.1), ('comes', 0.1), ('world', 0.09), ('script', 0.09), ('is', 0.09), ('however', 0.09), ('got', 0.09), ('br', 0.09), ('be', 0.09), ('all', 0.09), ('should', 0.08), ('life', 0.08), ('how', 0.07), ('her', 0.07), ('ever', 0.07), ('would', 0.06), ('this', 0.06), ('of', 0.06), ('me', 0.06), ('in', 0.06), ('even', 0.06), ('don', 0.06), ('and', 0.06), ('so', 0.05), ('if', 0.05), ('from', 0.05), ('by', 0.05), ('at', 0.05), ('an', 0.05), ('one', 0.04), ('to', 0.03), ('that', 0.03), ('it', 0.03)]\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "X = tfidf.fit_transform(C)\n",
    "\n",
    "print(sorted([(feature_names[i], round(X[10,i], 2)) for i in X[10].nonzero()[1]], key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eb348c",
   "metadata": {},
   "source": [
    "## 1.3 Training\n",
    "Next, we train a text classifier on the preprocessed training data. We're going to experiment with a classic text classification model: Logistic Regression.\n",
    "\n",
    "[Logistic Regression models](https://en.wikipedia.org/wiki/Logistic_regression) model the log-odds $l$, or $log(p/(1-p))$, of a class as a linear model and estimate the parameters $\\beta$ of the model during training: \n",
    "\n",
    "\\begin{equation*}\n",
    "l = \\beta_0 + \\sum_{i=1}^n \\beta_i x_i\n",
    "\\end{equation*}\n",
    "\n",
    "These models often achieve great performance in text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a316da",
   "metadata": {},
   "source": [
    "### Model selection\n",
    "\n",
    "It also might be appropriate to add regularization to a Logistic Regression model, to avoid overfitting.\n",
    "In Scikit-Learn, we can add a regularization of type L2 through the parameter `C`. This means the function we are optimizing is a bit different:\n",
    "\n",
    "\\begin{equation*}\n",
    "l = \\beta_0 + \\sum_{i=1}^n \\beta_i x_i + \\frac{1}{C}\\sum_{i=1}^n \\beta_i^2\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb5d6af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance achieved an accuracy of 0.8038 with C=0.1.\n",
      "Model performance achieved an accuracy of 0.8492 with C=1.\n",
      "Model performance achieved an accuracy of 0.8612 with C=5.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "Cs = [0.1, 1, 5]\n",
    "for C in Cs:\n",
    "    model.C = C\n",
    "    scores = cross_val_score(model, X, train_target)\n",
    "    print(f\"Model performance achieved an accuracy of {scores.mean():.4f} with C={C}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0fd3f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=5, max_iter=200)\n",
    "model.fit(X, train_target);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab29dd5",
   "metadata": {},
   "source": [
    "## 1.4 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c15c37cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_test = cv.transform(test_documents)\n",
    "X_test = tfidf.transform(C_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0af12867",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = test_target\n",
    "pred_labels = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5aca7291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2184  310]\n",
      " [ 284 2222]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a9fb970-5d69-4261-a8c0-0631b040de01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "ax = sns.heatmap(cm, annot=True, cmap='BuPu', fmt='g', cbar=False)\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "ax.xaxis.set_ticklabels(['True','False'])\n",
    "ax.yaxis.set_ticklabels(['True','False']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d83dfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8812\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b45b322",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
